{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Class08e-xgboost_default_and_tuned.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timcsmith/MIS536-Public/blob/master/Notebooks/Class08e_xgboost_default_and_tuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxGJbLsUhuc8"
      },
      "source": [
        "# Class08e - Prediction using XGBoost(using default parameters and with with Hyperparameter Tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tuXRZKEYrDa"
      },
      "source": [
        "## Introduction and Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q08EVUytY3eh"
      },
      "source": [
        "\n",
        "In this project, we reuse the [UCI's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/census+income) census data.\n",
        "\n",
        "The first section of this exercise is a duplicate of the previous Class08-random-forest-default.ipynb. The second section demonstrates how we can use hyper parameter tuning techniques to identify the best performing parameters for the given model (in this case, Random Forests).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2A_u7rQhuc_"
      },
      "source": [
        "\n",
        "# Section 1: Prediction with Random Forests (using default parameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmYLcm3aY8X5"
      },
      "source": [
        "## Step 1: Install and import necessary packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYulo3PeZvs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zNdljvIhuc8"
      },
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmP3fttHqieO"
      },
      "source": [
        "## Step 2: Load, clean and prepare data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGgrXNQPZT3J"
      },
      "source": [
        "### 2.1 Read data (income.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3u5LsGyhudA"
      },
      "source": [
        "income_df = pd.read_csv(\"https://raw.githubusercontent.com/timcsmith/MIS536-Public/master/Data/income.csv\", engine='python', delimiter=\", \")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aOH_GFGZZFx"
      },
      "source": [
        "### 2.2 Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkUM_mnHhudC",
        "outputId": "118a8e63-f90c-46e8-9ca1-dce42af23568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# Explore the dataset\n",
        "# read the first row of the dataset \n",
        "print(income_df.head())\n",
        "print(income_df.columns)\n",
        "print(income_df.describe())\n",
        "print(income_df.info())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   age         workclass  fnlwgt  ... hours-per-week  native-country income\n",
            "0   39         State-gov   77516  ...             40   United-States  <=50K\n",
            "1   50  Self-emp-not-inc   83311  ...             13   United-States  <=50K\n",
            "2   38           Private  215646  ...             40   United-States  <=50K\n",
            "3   53           Private  234721  ...             40   United-States  <=50K\n",
            "4   28           Private  338409  ...             40            Cuba  <=50K\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
            "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
            "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
            "       'income'],\n",
            "      dtype='object')\n",
            "                age        fnlwgt  ...  capital-loss  hours-per-week\n",
            "count  32561.000000  3.256100e+04  ...  32561.000000    32561.000000\n",
            "mean      38.581647  1.897784e+05  ...     87.303830       40.437456\n",
            "std       13.640433  1.055500e+05  ...    402.960219       12.347429\n",
            "min       17.000000  1.228500e+04  ...      0.000000        1.000000\n",
            "25%       28.000000  1.178270e+05  ...      0.000000       40.000000\n",
            "50%       37.000000  1.783560e+05  ...      0.000000       40.000000\n",
            "75%       48.000000  2.370510e+05  ...      0.000000       45.000000\n",
            "max       90.000000  1.484705e+06  ...   4356.000000       99.000000\n",
            "\n",
            "[8 rows x 6 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32561 entries, 0 to 32560\n",
            "Data columns (total 15 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   age             32561 non-null  int64 \n",
            " 1   workclass       32561 non-null  object\n",
            " 2   fnlwgt          32561 non-null  int64 \n",
            " 3   education       32561 non-null  object\n",
            " 4   education-num   32561 non-null  int64 \n",
            " 5   marital-status  32561 non-null  object\n",
            " 6   occupation      32561 non-null  object\n",
            " 7   relationship    32561 non-null  object\n",
            " 8   race            32561 non-null  object\n",
            " 9   sex             32561 non-null  object\n",
            " 10  capital-gain    32561 non-null  int64 \n",
            " 11  capital-loss    32561 non-null  int64 \n",
            " 12  hours-per-week  32561 non-null  int64 \n",
            " 13  native-country  32561 non-null  object\n",
            " 14  income          32561 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 3.7+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiaaNFX2Zf-I"
      },
      "source": [
        "### 2.3 Clean/transform data (where necessary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JuJlVGDkINJ",
        "outputId": "89561aa6-895a-4e64-cc3a-8f94ac134bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# based on findings from data exploration, we need to clean up colum names, as there are some leading whitespace characters\n",
        "income_df.columns = [s.strip() for s in income_df.columns] \n",
        "income_df.columns"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
              "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
              "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
              "       'income'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyUKMZqohudE",
        "outputId": "dfd47bed-040f-4742-a143-c547d8aaec3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# clean the datast: sex is not numeric.\n",
        "income_df.sex = income_df.sex.replace(\"Male\", 0, regex=True)\n",
        "income_df.sex = income_df.sex.replace(\"Female\", 1, regex=True)\n",
        "income_df.sex"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        1\n",
              "        ..\n",
              "32556    1\n",
              "32557    0\n",
              "32558    1\n",
              "32559    0\n",
              "32560    1\n",
              "Name: sex, Length: 32561, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7_FPefb34E3"
      },
      "source": [
        "# Transform our predictors into integers. This is necessary if we later want to test precision and recall. \n",
        "income_df.income.unique()\n",
        "income_df.income = income_df.income.replace(\"<=50K\", 0, regex=True)\n",
        "income_df.income = income_df.income.replace(\">50K\", 1, regex=True)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKY30W1pZxCP"
      },
      "source": [
        "## Step 3 Split data intro training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0fAfB0ThudG",
        "outputId": "17aa5ef2-425b-4fc3-e808-102af768bb21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# construct datasets for analysis\n",
        "target = 'income'\n",
        "predictors = ['age', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "X = income_df[predictors]\n",
        "y = income_df[target]\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       age  sex  capital-gain  capital-loss  hours-per-week\n",
            "0       39    0          2174             0              40\n",
            "1       50    0             0             0              13\n",
            "2       38    0             0             0              40\n",
            "3       53    0             0             0              40\n",
            "4       28    1             0             0              40\n",
            "...    ...  ...           ...           ...             ...\n",
            "32556   27    1             0             0              38\n",
            "32557   40    0             0             0              40\n",
            "32558   58    1             0             0              40\n",
            "32559   22    0             0             0              20\n",
            "32560   52    1         15024             0              40\n",
            "\n",
            "[32561 rows x 5 columns]\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "32556    0\n",
            "32557    1\n",
            "32558    0\n",
            "32559    0\n",
            "32560    1\n",
            "Name: income, Length: 32561, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0DkCAoChudI",
        "outputId": "a19d14e9-7ff2-4999-9013-68a56c2be5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# create the training set and the test set \n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X,y, test_size=0.3, random_state=1)\n",
        "print(train_X)\n",
        "print(valid_X)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       age  sex  capital-gain  capital-loss  hours-per-week\n",
            "16525   44    0             0             0              60\n",
            "14551   22    1             0             0              30\n",
            "518     21    1             0             0              35\n",
            "22524   46    0             0             0              40\n",
            "11425   17    0             0             0              20\n",
            "...    ...  ...           ...           ...             ...\n",
            "32511   25    1             0             0              40\n",
            "5192    32    0         15024             0              45\n",
            "12172   27    0             0             0              40\n",
            "235     59    0             0             0              40\n",
            "29733   33    0             0          1902              45\n",
            "\n",
            "[22792 rows x 5 columns]\n",
            "       age  sex  capital-gain  capital-loss  hours-per-week\n",
            "9646    62    1             0             0              66\n",
            "709     18    0             0             0              25\n",
            "7385    25    0         27828             0              50\n",
            "16671   33    0             0             0              40\n",
            "21932   36    1             0             0              40\n",
            "...    ...  ...           ...           ...             ...\n",
            "29663   61    0         15024             0              40\n",
            "29310   22    1             0             0              40\n",
            "29661   37    1          5013             0              40\n",
            "19491   23    0             0             0              40\n",
            "2861    44    1             0             0              40\n",
            "\n",
            "[9769 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSk0zea3qnoz"
      },
      "source": [
        "## Step 4: Create and train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gNRdX-qtNT"
      },
      "source": [
        "You can find details about the XGBoostClassifier [here]https://xgboost.readthedocs.io/en/latest/parameter.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbPfUmcEXf6K"
      },
      "source": [
        "### 4.1 Create a random forest using all of the default parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ60Vn1AhudK"
      },
      "source": [
        "xgboost=XGBClassifier(random_state=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntgxBhvJXkjp"
      },
      "source": [
        "### 4.2 Fit the model to the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPL4rRlVhudM",
        "outputId": "71a55913-fc76-46fc-b6f9-af2c5745c503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "xgboost.fit(train_X, train_y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=1,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMvD4-9wXy_1"
      },
      "source": [
        "### 4.3 Review of the performance of the model on the validation/test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAcO31dIX7JE",
        "outputId": "97c316a7-2f26-45fb-9ca1-1050a148d754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "validation_predictions = xgboost.predict(valid_X)\n",
        "\n",
        "print(confusion_matrix(valid_y, validation_predictions))\n",
        "print(accuracy_score(valid_y, validation_predictions))\n",
        "print(precision_score(valid_y, validation_predictions))\n",
        "print(recall_score(valid_y, validation_predictions))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7429  121]\n",
            " [1476  743]]\n",
            "0.8365236974101751\n",
            "0.8599537037037037\n",
            "0.3348355114916629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HLaMDTwaN9H"
      },
      "source": [
        "## Step 5: Deploy model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOnuFI0gaVmd"
      },
      "source": [
        "In this exercise (predicting income), there is no model deployment. Here we develop a model and test is performance on the validation data. \n",
        "\n",
        "What does \"deploying\" a model mean? Up to this point, we've trained a model to our training data, and then estimated the performance of this model on new data by testing it's performance on validation data. \n",
        "\n",
        "In this course, we often stop after building the model. In practice, the model is being built to be \"used\" in some fashion. Using the model is often referred to as \"deploying\" the model. \n",
        "\n",
        "How a model is deployed can vary. It may simply be deployed as a notebook that reads latest predictor data and uses the develped model to make predictions. Or, the model can be deployed inside enterprise decision support software that automatically makes predictions on incoming data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EouSfy5vIYm8"
      },
      "source": [
        "# Section 2: Prediction with XGBoost (using hyperparmater tuning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA3Yw37NeyaU"
      },
      "source": [
        "This section demonstrates how to refine the performance of our XGBoost model using hyperparameter tuning techniques. \n",
        "\n",
        "This section doesn't duplicate the data loading, cleaning, and splitting of the first section. This section shows how to create and test a random forest classifier using RandomSearchCV and GridSearchCV techniques. \n",
        "\n",
        "Both RandomeSearchCV and GridSearchCV test different model parameters. These help to determine the parameters that produce the best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqWYH75ZgZRA"
      },
      "source": [
        "## Step 1: Determine the parameters that can be \"tuned\"\n",
        "\n",
        "You can review the parameters of the model which you're trying to \"tune\".\n",
        "\n",
        "Key tuning parameters are:\n",
        "\n",
        "* n_estimators\n",
        "* early_stopping_round\n",
        "* learning_rate\n",
        "\n",
        "If you wish, you can review the parameters for this model found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUIGBpt9gytG"
      },
      "source": [
        "When reviewing these parameters (and understanding something about RandomForests), we can identify the following parameters that could affect model fit. \n",
        "\n",
        "* n_estimators\n",
        "* criterion\n",
        "* max_depth\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "* max_features\n",
        "* max_leaf_nodes\n",
        "* min_impurity_decrease\n",
        "* bootstrap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1w-3ddohW8S"
      },
      "source": [
        "### Step 2: Create an initial 'wide' range of possible hyperparameter values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAal78lyhhEx"
      },
      "source": [
        "Here we create a wide range of possible parameter values for each of the hyperparameters for this model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7C9BvWWIbfg"
      },
      "source": [
        "# Like with k-nn and decision trees, there are a number of hyperparamaters that can be explored \n",
        "\n",
        "# Number of trees in random forest; default is 100\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "# Maximum number of levels in tree. If None, then nodes are expanded until all leaves are pure or until all \n",
        "# leaves contain less than min_samples_split samples.\n",
        "# default = None\n",
        "max_depth = [int(x) for x in np.linspace(1, 200, num = 199)]\n",
        "max_depth.append(None)\n",
        "\n",
        "\n",
        "\n",
        "# Create the random grid\n",
        "param_grid_random = {'n_estimators': n_estimators,\n",
        "                      'max_depth': max_depth,\n",
        "                      'learning_rate': learning_rate,\n",
        "                     }\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOlUlWyTjFt3"
      },
      "source": [
        "### Step 3: Use Randomize Search to narrow the possible range of parameter values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9-MlOV1MHyr",
        "outputId": "ebe4370d-5762-497e-d1f6-bc21d8b9b1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Use the param_grid_random for an initial \"rough\" search using Randomized search\n",
        "xgb = XGBClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "randomSearch = RandomizedSearchCV(estimator = xgb, param_distributions = param_grid_random, n_iter = 300, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "#randomSearch.fit(train_X, train_y,early_stopping_rounds=10)\n",
        "randomSearch.fit(train_X, train_y)\n",
        "bestRandomModel = randomSearch.best_estimator_\n",
        "print('Best parameters found: ', randomSearch.best_params_)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done 568 tasks      | elapsed: 10.4min\n",
            "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 16.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGkze5JWjkAL"
      },
      "source": [
        "### Step 4: Test the performance of the selected parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baZShx5VXIr9",
        "outputId": "02acef00-dd57-4045-e0e4-8315b2a852c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "validation_predictions = bestRandomModel.predict(valid_X)\n",
        "print('Accuracy Score: ', accuracy_score(valid_y, validation_predictions))\n",
        "print('Precision Score: ', precision_score(valid_y, validation_predictions))\n",
        "print('Recall Score: ', recall_score(valid_y, validation_predictions))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score:  0.8378544375063978\n",
            "Precision Score:  0.7697536108751062\n",
            "Recall Score:  0.40829202343397925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAqjsBT5jzw9"
      },
      "source": [
        "### Step 5: Use knowledge gained from Step 3 and 4 to create new 'narrow' range of possible hyperparameter values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnKODm5APuXc"
      },
      "source": [
        "# let's take the best parameters from the the random search, and use this as a base for gridsearch\n",
        "param_grid =  {'n_estimators': [180, 190, 195, 200, 205, 210, 220],\n",
        "                'max_depth': [2,3, 4, 5, 6],\n",
        "                'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
        "              }"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON2ugTU4kRFV"
      },
      "source": [
        "### Step 6: Use Grid (exhaustive) to refine model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtLzreQIQtXH",
        "outputId": "23f16192-9979-41b9-ff00-d06d00a13ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# refine our search using param_grid\n",
        "xgb = XGBClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "gridSearch = GridSearchCV(estimator = xgb, param_grid=param_grid, cv = 3, verbose=2,  n_jobs = -1)\n",
        "# Fit the random search model\n",
        "gridSearch.fit(train_X, train_y)\n",
        "bestGridModel = gridSearch.best_estimator_\n",
        "print('Best parameters found: ', gridSearch.best_params_)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 175 candidates, totalling 525 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed:   16.8s\n",
            "[Parallel(n_jobs=-1)]: Done 525 out of 525 | elapsed:   29.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 210}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAv6LTYVkhA_"
      },
      "source": [
        "### Step 7: Test the performance of the model using identified parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0fWR0cvNiec",
        "outputId": "f9370101-65cb-40cb-8998-c02afb7bfdd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "validation_predictions = bestGridModel.predict(valid_X)\n",
        "print(accuracy_score(valid_y, validation_predictions))\n",
        "print(precision_score(valid_y, validation_predictions))\n",
        "print(recall_score(valid_y, validation_predictions))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8379568021291841\n",
            "0.7694915254237288\n",
            "0.40919333032897703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toa5Xyg2lzDZ"
      },
      "source": [
        "### Step 8: If necessary, repeat steps 5 through 7 with more granular and targeted parameter search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9BhtBG0lA9V"
      },
      "source": [
        "Continuing to tune your model is optional, but if there is evidence that there is room for continued improvement in the model, use the information from Step 6 to create a more refined set of parameter ranges and rerun the GridSearchCV. \n",
        "\n",
        "In this case, we increased accuracy from 0.8379 to 0.8380. The importance of this increase is context-dependent. In some contexts, every little increase is worth the effort -- in other contexts, such increases would have minimal to no benefit. In this case, we'll assume that any incremental increase in performance from repeated searching is not enough to justify continuing. \n",
        "\n",
        "NOTE: The decision to continue further is dependent on the expected incremental increase in performance, the importance of any small incremental increase (to the business), and the resources you have (time and computing power). Recognize that you generally have a \"diminishing\" returns situation, where it takes an increasingly large amount of computation (training) to gain an increasingly smaller increase in performance. "
      ]
    }
  ]
}